{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzljerry/Blockchain-based-Edge-Resource-Sharing-for-Metaverse/blob/main/QL_Task_Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGlrRof1AKqm"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "'''\n",
        "@Title : Q_learning based solution of mutiple knapsacks problem\n",
        "@Author : Zhilin Wang\n",
        "@Email : wangzhil@iu.edu\n",
        "@Date : 14-04-2022\n",
        "'''\n",
        "#import tensorflow as tf\n",
        "#tf.test.gpu_device_name()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from random import choice\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aY4XdHSms7F"
      },
      "outputs": [],
      "source": [
        "#generate data\n",
        "class DataCollection:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  #get random value\n",
        "  def generate_random_value(self,low,up,num):\n",
        "    randomlist = []\n",
        "    for i in range(num):\n",
        "      n = random.randint(low,up)\n",
        "      randomlist.append(n)\n",
        "    return randomlist\n",
        "\n",
        "  #generate task\n",
        "  def generate_task(self,p,D,T,num):\n",
        "    task=[]\n",
        "    for i in range(num):\n",
        "      a=[p[i],D[i],T[i]]\n",
        "      task.append(a)\n",
        "    return task\n",
        "  \n",
        "  #generate server\n",
        "  def generate_server(self,low,up,num):\n",
        "    server=self.generate_random_value(low,up,num)\n",
        "    return server\n",
        "\n",
        "  #generate group\n",
        "  def generate_group(self,num_task,num_server):\n",
        "    num=[i for i in range(num_task)]\n",
        "    shuffle(num)\n",
        "    group_task=np.array_split(num,num_server)\n",
        "    for i in range(num_server):\n",
        "      group_task[i]=list(group_task[i])\n",
        "    return group_task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BbfyeV7I1Na"
      },
      "outputs": [],
      "source": [
        "class Task_allocation:\n",
        "  # parameters\n",
        "  #task is a matrix which contains [price p_j, data size D_j, executing time t_e]\n",
        "  #server is a list, which contains the computational powers for each server\n",
        "  def __init__(self,f_j, d_i, alpha, B, P_i, G_i, R_i_j,delta, task,server,ser_task,ALPHA,GAMMA,EPSILON,MAX_EPISODES):\n",
        "    self.task=task\n",
        "    self.lamda=0.01\n",
        "    self.ser_task=ser_task\n",
        "    self.server=server\n",
        "    self.m=task.shape[0] #number of tasks\n",
        "    self.n=len(server) #number of servers\n",
        "    self.B=B #bandwidth\n",
        "    self.delta=0.001 #gussian noise\n",
        "    self.alpha=alpha #CPU parameter\n",
        "    self.P_i=P_i #trainsimission power\n",
        "    self.G_i=G_i #trainsimission gain\n",
        "    self.f_j=f_j #CPU frequency\n",
        "    self.d_i=d_i # sample cpu cyecles\n",
        "    self.R_i_j=R_i_j\n",
        "    self.N_STATES = self.m   # number of states\n",
        "    self.ACTIONS = [i for i in range(self.n)]     # action\n",
        "    self.EPSILON = EPSILON  # greedy\n",
        "    self.ALPHA = ALPHA     # learning rate\n",
        "    self.GAMMA = GAMMA    # discount\n",
        "    self.MAX_EPISODES = MAX_EPISODES\n",
        "    self.q_table=self.build_q_table()# q_table\n",
        "\n",
        "  #generate values which can be repeated\n",
        "  def generate_random_value(self,low,up,num):\n",
        "    randomlist = []\n",
        "    for i in range(num):\n",
        "      n = random.randint(low,up)\n",
        "      randomlist.append(n)\n",
        "    return randomlist  \n",
        "\n",
        "  # create the q_table with initial value=0\n",
        "  def build_q_table(self):\n",
        "    q_table = pd.DataFrame(\n",
        "        np.zeros((self.N_STATES, len(self.ACTIONS))),     # initialize q_table\n",
        "        columns=self.ACTIONS,    # columns, the name of actions\n",
        "    )\n",
        "    return q_table\n",
        "  \n",
        "  #check whether the server i can process task j with cpu and time constraints\n",
        "  def check_avaliable(self,j,i,h):\n",
        "    res=0\n",
        "    if self.task[j][1]*self.d_i[i]<self.server[i]: #cpu\n",
        "      if (self.task[j]*self.d_i[i]/self.f_j[i]+self.task[j][1]/self.R_i_j[h]).all()<self.task[j][2]: #time\n",
        "        res=1\n",
        "    return res\n",
        "\n",
        "  #if j is from i, then no transimision\n",
        "  def check_avaliable_1(self,j,i):\n",
        "    res=0\n",
        "    if self.task[j][1]*self.d_i[i]<self.server[i]: #cpu\n",
        "      if (self.task[j]*self.d_i[i]/self.f_j[i]).all()<self.task[j][2]:#time\n",
        "          res=1\n",
        "    return res\n",
        "\n",
        "\n",
        "  def find_source(self,j):\n",
        "    for com in self.ser_task:\n",
        "      if j in com:\n",
        "        index = com.index(j) \n",
        "    return index\n",
        "\n",
        "  #check whether task j is from server i\n",
        "  def check_source(self,task_num,group_num):\n",
        "    group=self.ser_task[group_num] # group\n",
        "    res=0\n",
        "    if task_num in group: #in or not\n",
        "      res=1\n",
        "    return res\n",
        "\n",
        "  #get the reward based on sources and avaliability\n",
        "  def get_feedback(self):\n",
        "    rew=[]\n",
        "    for j in range(self.m): # for each task\n",
        "      for i in range(self.n): # for each server\n",
        "        if self.check_source(j,i)==1: # in that group\n",
        "          #h=i\n",
        "          if self.check_avaliable_1(j,i)==1: # avaliable\n",
        "            reward=self.task[j][0]*self.task[j][1]*self.d_i[i] \n",
        "            -self.alpha*self.task[j][1]*self.d_i[i]*self.f_j[i]**2 # only need to compute\n",
        "          else:\n",
        "            reward=-self.task[j][1]/self.R_i_j[i]*self.P_i[i]+self.lamda*self.task[j][0]*self.task[j][1]*self.d_i[i] # only need to transimist\n",
        "        if self.check_source(j,i)!=1: # not in that group\n",
        "          h=self.find_source(j)\n",
        "          if self.check_avaliable(j,i,h)==1: # avaliable\n",
        "            reward=self.task[j][0]*self.task[j][1]*self.d_i[i]\n",
        "            -self.alpha*self.task[j][1]*self.d_i[i]*self.f_j[i]**2-self.lamda*self.task[j][0]*self.task[j][1]*self.d_i[i] # needs to compute\n",
        "          else:\n",
        "            reward=0 # get nothing because it can't process task j\n",
        "        rew.append(reward) # using a list to contain all the rewards for each task and each server\n",
        "    return rew\n",
        "\n",
        "  #to get the avaliable actions set for each task\n",
        "  def check_action_reward(self,task_num):\n",
        "    reward=self.get_feedback()\n",
        "    reward=np.array(reward).reshape(self.m,self.n) # using a m*n matrix to contain the rewards\n",
        "    index_list=[] \n",
        "    for i in range(self.n): #for each server\n",
        "      if reward[task_num][i]!=0: # if =0, the server can't process that task\n",
        "        action_index=i\n",
        "        index_list.append(action_index)\n",
        "    return index_list # return the avaliable index of actions\n",
        "\n",
        "\n",
        "    #to get the avaliable actions set for each task without speed up\n",
        "  def check_action_reward_without_speed_up(self,task_num):\n",
        "    reward=self.get_feedback()\n",
        "    reward=np.array(reward).reshape(self.m,self.n) # using a m*n matrix to contain the rewards\n",
        "    index_list=[] \n",
        "    for i in range(self.n): #for each server\n",
        "      #if reward[task_num][i]!=0: # if =0, the server can't process that task\n",
        "      action_index=i\n",
        "      index_list.append(action_index)\n",
        "    return index_list # return the avaliable index of actions\n",
        "\n",
        "  def select_action(self,task_num,state_actions):\n",
        "    #state_actions = self.check_action_reward(task_num)\n",
        "    if (np.random.uniform() > self.EPSILON) or (len(state_actions) == 0): \n",
        "    #if np.random.uniform() > self.EPSILON:  # not greedy\n",
        "      #action_name = np.random.choice(state_actions)\n",
        "      action_name =choice(state_actions)\n",
        "\n",
        "    else:\n",
        "      q_value=pd.DataFrame(self.q_table.loc[task_num,state_actions]).T\n",
        "      action_name = int(q_value.idxmax(axis=1))   # greedy\n",
        "    return action_name\n",
        "  \n",
        "  #create a table to contain the cpu capacity\n",
        "  def cpu_table(self):\n",
        "    cpu_table=self.build_q_table()\n",
        "    return cpu_table\n",
        "  \n",
        "  #create a table to contain the time\n",
        "  def time_table(self):\n",
        "    time_table=self.build_q_table()\n",
        "    return time_table\n",
        "\n",
        "\n",
        "  def check_action_valid(self,j,actions_ava,rewards,cpu_table,time_table,state_actions,action,acc_mu,acc_t):\n",
        "    res=True\n",
        "    if action in actions_ava: # select actions in avaliable sets\n",
        "      if (acc_mu>self.server[action] or acc_t< self.task[j][2]): # satisify that the cpu and time are both enough\n",
        "        state_actions.remove(action) # remove that action\n",
        "        action=self.select_action(j,state_actions)\n",
        "        if (acc_mu>self.server[action] or acc_t< self.task[j][2]):\n",
        "          action=self.check_action_valid(j,actions_ava,rewards,cpu_table,time_table,state_actions,action,acc_mu,acc_t)\n",
        "      else:\n",
        "        actions_ava.append(action)\n",
        "    return action\n",
        "\n",
        "\n",
        "  def q_update_modified(self):\n",
        "    actions_ava=[]\n",
        "    rews=[]\n",
        "    act=[]\n",
        "    rewards=np.array(self.get_feedback()).reshape(self.m,self.n)\n",
        "    cpu_table=self.cpu_table()\n",
        "    time_table=self.time_table()\n",
        "    # check the limitation of time and cpu\n",
        "    for j in range(self.m):\n",
        "      state_actions = self.check_action_reward(j)\n",
        "      action=self.select_action(j,state_actions)\n",
        "      #initialize acc_t and acc_mu\n",
        "      acc_mu=cpu_table[action].sum() # sum of cpu\n",
        "      acc_t=(self.server[action]-acc_mu)/self.f_j[action]\n",
        "      action=self.check_action_valid(j,actions_ava,rewards,cpu_table,time_table,state_actions,action,acc_mu,acc_t)\n",
        "\n",
        "      # record the cpu and time\n",
        "      cpu_table[action][j]=self.task[j][1]\n",
        "      acc_mu=cpu_table[action].sum() # sum of cpu\n",
        "\n",
        "      #update q\n",
        "      if j != self.m-1:\n",
        "        self.q_table.iloc[j,action] += self.ALPHA*(rewards[j][action]\n",
        "                                                   +self.GAMMA*np.max(self.q_table.iloc[j+1,state_actions])-self.q_table.iloc[j,action])\n",
        "      else:\n",
        "        self.q_table.iloc[j,action] += self.ALPHA*(rewards[j][action]\n",
        "                                                 +self.GAMMA*np.max(self.q_table.iloc[j,state_actions])-self.q_table.iloc[j,action])\n",
        "      rew=rewards[j][action]# the rewards of each step\n",
        "      rews.append(rew) # rewads for all the tasks\n",
        "      act.append(action) #action set\n",
        "    res=sum(rews)#total rewards for one tempt\n",
        "    return self.q_table,res,act\n",
        "\n",
        "  #update q_table\n",
        "  def q_update(self):\n",
        "    actions_ava=[]\n",
        "    rews=[]\n",
        "    act=[]\n",
        "    rewards=np.array(self.get_feedback()).reshape(self.m,self.n)\n",
        "    cpu_table=self.cpu_table()\n",
        "    time_table=self.time_table()\n",
        "    # check the limitation of time and cpu\n",
        "    for j in range(self.m):\n",
        "      state_actions = self.check_action_reward(j)\n",
        "      action=self.select_action(j,state_actions)\n",
        "      #initialize acc_t and acc_mu\n",
        "      acc_mu=cpu_table[action].sum() # sum of cpu\n",
        "      acc_t=(self.server[action]-acc_mu)/self.f_j[action]\n",
        "      if action in actions_ava: # select actions in avaliable sets\n",
        "        if (acc_mu>self.server[action] or acc_t< self.task[j][2]): # satisify that the cpu and time are both enough\n",
        "          state_actions.remove(action) # remove that action\n",
        "          action=self.select_action(j,state_actions) # reselect another action\n",
        "      else:\n",
        "        actions_ava.append(action)\n",
        "\n",
        "      # record the cpu and time\n",
        "      cpu_table[action][j]=self.task[j][1]\n",
        "      acc_mu=cpu_table[action].sum() # sum of cpu\n",
        "\n",
        "      #update q\n",
        "      if j != self.m-1:\n",
        "        self.q_table.iloc[j,action] += self.ALPHA*(rewards[j][action]\n",
        "                                                   +self.GAMMA*np.max(self.q_table.iloc[j+1,state_actions])-self.q_table.iloc[j,action])\n",
        "      else:\n",
        "        self.q_table.iloc[j,action] += self.ALPHA*(rewards[j][action]\n",
        "                                                 +self.GAMMA*np.max(self.q_table.iloc[j,state_actions])-self.q_table.iloc[j,action])\n",
        "      rew=rewards[j][action]# the rewards of each step\n",
        "      rews.append(rew) # rewads for all the tasks\n",
        "      act.append(action) #action set\n",
        "    res=sum(rews)#total rewards for one tempt\n",
        "    return self.q_table,res,act\n",
        "\n",
        "    #update q_table \n",
        "  def q_update_without_speed_up(self):\n",
        "    actions_ava=[]\n",
        "    rews=[]\n",
        "    act=[]\n",
        "    rewards=np.array(self.get_feedback()).reshape(self.m,self.n)\n",
        "    cpu_table=self.cpu_table()\n",
        "    time_table=self.time_table()\n",
        "    # check the limitation of time and cpu\n",
        "    for j in range(self.m):\n",
        "      state_actions = self.check_action_reward_without_speed_up(j)\n",
        "      action=self.select_action(j,state_actions)\n",
        "      #initialize acc_t and acc_mu\n",
        "      acc_mu=cpu_table[action].sum() # sum of cpu\n",
        "      acc_t=(self.server[action]-acc_mu)/self.f_j[action]\n",
        "      action=self.check_action_valid(j,actions_ava,rewards,cpu_table,time_table,state_actions,action,acc_mu,acc_t)\n",
        "      # record the cpu and time\n",
        "      cpu_table[action][j]=self.task[j][1]\n",
        "      acc_mu=cpu_table[action].sum() # sum of cpu\n",
        "\n",
        "      #update q\n",
        "      if j != self.m-1:\n",
        "        self.q_table.iloc[j,action] += self.ALPHA*(rewards[j][action]\n",
        "                                                   +self.GAMMA*np.max(self.q_table.iloc[j+1,state_actions])-self.q_table.iloc[j,action])\n",
        "      else:\n",
        "        self.q_table.iloc[j,action] += self.ALPHA*(rewards[j][action]\n",
        "                                                 +self.GAMMA*np.max(self.q_table.iloc[j,state_actions])-self.q_table.iloc[j,action])\n",
        "      rew=rewards[j][action]# the rewards of each step\n",
        "      rews.append(rew) # rewads for all the tasks\n",
        "      act.append(action) #action set\n",
        "    res=sum(rews)#total rewards for one tempt\n",
        "    return self.q_table,res,act\n",
        "  \n",
        "  #get the index based on greedy\n",
        "  def greedy_reward(self,j,rewards):\n",
        "    rewards=pd.DataFrame(rewards)\n",
        "    index=self.check_action_reward(j)\n",
        "    reward=rewards.iloc[j,index]\n",
        "    max_reward=max(list(reward))\n",
        "    max_index=list(reward).index(max_reward)\n",
        "    return max_index\n",
        "\n",
        "  #get the rewards by greedy search\n",
        "  def greedy_select(self):\n",
        "    actions_ava=[]\n",
        "    rews=[]\n",
        "    act=[]\n",
        "    rewards=np.array(self.get_feedback()).reshape(self.m,self.n)\n",
        "    cpu_table=self.cpu_table()\n",
        "    time_table=self.time_table()\n",
        "    # check the limitation of time and cpu\n",
        "    for j in range(self.m):\n",
        "      state_actions = self.check_action_reward(j)\n",
        "      action=self.greedy_reward(j,rewards)\n",
        "      #initialize acc_t and acc_mu\n",
        "      acc_mu=cpu_table[action].sum() # sum of cpu\n",
        "      acc_t=(self.server[action]-acc_mu)/self.f_j[action]\n",
        "      if action in actions_ava: # select actions in avaliable sets\n",
        "        if (acc_mu>self.server[action] or acc_t< self.task[j][2]): # satisify that the cpu and time are both enough\n",
        "          state_actions.remove(action) # remove that action\n",
        "          action=self.select_action(j,state_actions) # reselect another action\n",
        "      else:\n",
        "        actions_ava.append(action)\n",
        "\n",
        "      # record the cpu and time\n",
        "      cpu_table[action][j]=self.task[j][1]\n",
        "      acc_mu=cpu_table[action].sum() # sum of cpu\n",
        "\n",
        "      res=rewards[j][action]\n",
        "      rews.append(res)\n",
        "    rews_sum=sum(rews)\n",
        "    return rews_sum\n",
        "\n",
        "  # get the index randomly selected  \n",
        "  def random_index(self,j,rewards):\n",
        "    rewards=pd.DataFrame(rewards)\n",
        "    #for j in range(self.m):\n",
        "    index=self.check_action_reward(j)\n",
        "    reward=rewards.iloc[j,index]\n",
        "    random_reward=choice(list(reward))\n",
        "    random_index=list(reward).index(random_reward)\n",
        "    return random_index\n",
        "\n",
        "  #get the rewards by random strategy\n",
        "  def ramdom_select(self):\n",
        "    actions_ava=[]\n",
        "    rews=[]\n",
        "    act=[]\n",
        "    rewards=np.array(self.get_feedback()).reshape(self.m,self.n)\n",
        "    cpu_table=self.cpu_table()\n",
        "    time_table=self.time_table()\n",
        "    # check the limitation of time and cpu\n",
        "    for j in range(self.m):\n",
        "      state_actions = self.check_action_reward(j)\n",
        "      action=self.random_index(j,rewards)\n",
        "      #initialize acc_t and acc_mu\n",
        "      acc_mu=cpu_table[action].sum() # sum of cpu\n",
        "      acc_t=(self.server[action]-acc_mu)/self.f_j[action]\n",
        "\n",
        "      if action in actions_ava: # select actions in avaliable sets\n",
        "        if (acc_mu>self.server[action] or acc_t< self.task[j][2]): # satisify that the cpu and time are both enough\n",
        "          state_actions.remove(action) # remove that action\n",
        "          action=self.select_action(j,state_actions) # reselect another action\n",
        "      else:\n",
        "        actions_ava.append(action)\n",
        "\n",
        "      # record the cpu and time\n",
        "      cpu_table[action][j]=self.task[j][1]\n",
        "      acc_mu=cpu_table[action].sum() # sum of cpu\n",
        "\n",
        "      res=rewards[j][action]\n",
        "      rews.append(res)\n",
        "    rews_sum=sum(rews)\n",
        "    return rews_sum\n",
        "  \n",
        "  #training with speed up\n",
        "  def training(self):\n",
        "    res=[]\n",
        "    act=[]\n",
        "    #training\n",
        "    for i in range(self.MAX_EPISODES):\n",
        "      q_table,reward,actions=self.q_update_modified()\n",
        "      res.append(reward)\n",
        "      act.append(actions)\n",
        "    max_reward=np.max(res)\n",
        "    best_solution=act[res.index(np.max(res))]\n",
        "    index_list=[j for j in range(self.m)]\n",
        "    best_server_task=list(zip(index_list,best_solution)) # the best allocation\n",
        "    return max_reward,best_server_task,q_table,res\n",
        "\n",
        "  #training without speed up\n",
        "  def training_without_speed_up(self):\n",
        "    res=[]\n",
        "    act=[]\n",
        "    #training\n",
        "    for i in range(self.MAX_EPISODES):\n",
        "      q_table,reward,actions=self.q_update_without_speed_up()\n",
        "      res.append(reward)\n",
        "      act.append(actions)\n",
        "    max_reward=np.max(res)\n",
        "    best_solution=act[res.index(np.max(res))]\n",
        "    index_list=[j for j in range(self.m)]\n",
        "    best_server_task=list(zip(index_list,best_solution)) # the best allocation\n",
        "    return max_reward,best_server_task,q_table,res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GppE1O6NrMrw",
        "outputId": "edde0a9a-5bd8-4a33-b53b-954ed460ab4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17396.0\n",
            "16107.0\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "  #parameters\n",
        "  num_server=20#number of servers\n",
        "  num_task=50#number of tasks\n",
        "  EPSILON=0.9 # greedy\n",
        "  GAMMA=0.9 #discount\n",
        "  ALPHA=0.01 #learning rate\n",
        "  max_iteration=500#iterations\n",
        "  #generate data\n",
        "  data=DataCollection()\n",
        "  p=data.generate_random_value(1,10,num_task) #price\n",
        "  D=data.generate_random_value(10,20,num_task) #data size\n",
        "  T=data.generate_random_value(1,100,num_task) #time\n",
        "  task=data.generate_task(p,D,T,num_task) #tasks\n",
        "  task=np.array(task) \n",
        "  ser_task=data.generate_group(num_task,num_server) #server-class group\n",
        "  server=data.generate_server(200,400,num_server) #server\n",
        "  f_j=data.generate_random_value(1,10,num_server) #CPU frequency\n",
        "  d_i=data.generate_random_value(1,10,num_server)\n",
        "  alpha=0.00001\n",
        "  B=data.generate_random_value(5,10,num_server)\n",
        "  P_i=data.generate_random_value(5,10,num_server)\n",
        "  G_i=data.generate_random_value(5,10,num_server)\n",
        "  delta=0.001\n",
        "  R_i_j=[]\n",
        "  for i in range(num_server):\n",
        "    R=B[i]*math.log(1+(P_i[i]*G_i[i])/delta**2)\n",
        "    R_i_j.append(R)\n",
        "\n",
        "  #training\n",
        "  #self,f_j, d_i, alpha, B, P_i, G_i, task,server,ser_task,ALPHA,GAMMA,EPSILON,MAX_EPISODES\n",
        "  ql=Task_allocation(f_j, d_i, alpha, B, P_i, G_i, R_i_j,delta,task,server,ser_task,EPSILON,ALPHA,GAMMA,max_iteration)\n",
        "  #random\n",
        "  max_random=ql.ramdom_select()\n",
        "  print(max_random)\n",
        "  #greedy\n",
        "  max_greedy=ql.greedy_select()\n",
        "  print(max_greedy)\n",
        "  #q_learning\n",
        "  max_reward,solution,q_table,res=ql.training()\n",
        "  solution=np.array(solution)\n",
        "  #print(solution)\n",
        "  print(max_reward)\n",
        "  print(res)\n",
        "  #print(q_table)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}